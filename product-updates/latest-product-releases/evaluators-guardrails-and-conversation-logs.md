---
description: Released in September 2025
layout:
  width: default
  title:
    visible: true
  description:
    visible: true
  tableOfContents:
    visible: true
  outline:
    visible: true
  pagination:
    visible: true
  metadata:
    visible: true
---

# Evaluators, Guardrails & Conversation Logs

## Evaluators & Guardrails

### ğŸ“Œ Overview

**Evaluators** (or evals) are automated tools in indigo.ai that **analyze conversations**.\
They help measure key aspects such as response relevance, user sentiment, tone, and topics, providing objective, timely, and scalable evaluations without relying on manual reviews.

Evaluators work together with **Guardrails**, which act as **preventive checks** during live conversations.&#x20;

Together, they provide a complete framework for monitoring, improving, and governing the virtual assistant's quality.

{% hint style="info" %}
From a technical perspective, evals run [LLM models](../../getting-started/ai-knowledge-hub/large-language-models-llms-available-on-our-platform.md) _after_ a response to check its quality and correctness, while guardrails run _before and during_ generation to constrain what the virtual assistant can or cannot say.

In simple terms:

* **Evals = post-checks** âœ… (evaluate answers after theyâ€™re produced).
* **Guardrails = pre-checks + live constraints**ğŸš¦ (control behavior before and during the reply).
{% endhint %}

<figure><img src="../../.gitbook/assets/add evaluator step 1.png" alt=""><figcaption></figcaption></figure>

### âœ… Benefits

* **Objective and fast evaluations** â†’ continuous quality monitoring, independent from human judgment.
* **Operational efficiency** â†’ reduce manual effort, freeing resources for higher-value tasks.
* **Trend and pattern detection** â†’ uncover recurring issues, user sentiment trends, or escalation needs.
* **Improved perceived quality** â†’ proactive monitoring strengthens brand reputation and service trustworthiness.

### Built-in vs Custom

indigo.ai provides both built-in evaluators and guardrails (ready-to-use â€œblack boxesâ€), and the **ability to design custom ones for specific use cases**.

{% hint style="info" %}
For more details read the full guide: [evaluators-and-guardrails.md](../../getting-started/workspace-activities/utilities/evaluators-and-guardrails.md "mention")
{% endhint %}

## Conversation Logs

### ğŸ“Œ Overview

Conversation Logs provide a centralized place in the indigo.ai platform where you can view the **full list of conversations and their corresponding Evaluator or Guardrail results**.

This section allows teams to analyze every single conversation, understand how evaluators and guardrails performed, and start any necessary **debugging** flows.

<figure><img src="../../.gitbook/assets/conversation log 3.png" alt=""><figcaption></figcaption></figure>

### âœ… Benefits

* **Detailed visibility** â†’ review all conversations in one place, with full evaluator and guardrail outcomes.
* **Operational efficiency** â†’ quickly identify issues, user feedback, or API errors without switching tools.
* **Debug-ready** â†’ inspect conversations in detail, including errors and triggers, to improve the chatbotâ€™s performance.

### ğŸ’¬ What is a Conversation Log?

A Conversation Log represents one interaction between an AI Agent and an end-user.

* A conversation begins when the user starts the chat and ends when they click Close Chat.
* Multiple conversations together form a chat.

In the Conversation Logs view, you can browse the list of conversations and check how evaluators and guardrails performed for each one.

{% hint style="info" %}
For more details read the full guide: [conversation-logs.md](../../getting-started/workspace-activities/utilities/conversation-logs.md "mention")
{% endhint %}
